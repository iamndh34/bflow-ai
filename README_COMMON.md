# BFlow AI - Common Documentation

TÃ i liá»‡u chung cho cáº£ **bflow_ai** (v1) vÃ  **bflow_ai_v2** (v2).

## ğŸ“‹ Tá»•ng quan

### bflow_ai (Version 1)
- **Architecture**: Pipeline-based vá»›i 8 steps
- **RAG**: Vector search (embeddings) + Hybrid similarity
- **LLM**: Ollama (qwen2.5)
- **Database**: MongoDB + Redis (cache)
- **Agents**: Multi-agent system (COA, Posting Engine, General)

### bflow_ai_v2 (Version 2)
- **Architecture**: FastAPI + LangChain + GraphRAG
- **RAG**: Knowledge graph-based (GraphRAG) + Local/Global search
- **LLM**: Ollama (qwen2.5) + LangChain integration
- **Graph**: NetworkX + Community detection
- **Vector Store**: ChromaDB (fallback)

## ğŸ”„ CÃ¡ch Switch Qua Láº¡i Backend

### CÃ¡ch 1: DÃ¹ng cÃ¹ng port 8010 (Khuyáº¿n nghá»‹)

```bash
# Stop backend hiá»‡n táº¡i (Ctrl+C)

# Cháº¡y bflow_ai (v1)
cd bflow_ai
uvicorn main:app --port 8010

# HOáº¶C cháº¡y bflow_ai_v2 (v2)
cd bflow_ai_v2
uvicorn app.main:app --port 8010
```

### CÃ¡ch 2: Cháº¡y song song 2 port

```bash
# Terminal 1: bflow_ai v1
cd bflow_ai
uvicorn main:app --port 8010

# Terminal 2: bflow_ai_v2 v2
cd bflow_ai_v2
uvicorn app.main:app --port 8011
```

Sau Ä‘Ã³ Ä‘á»•i port trong UI config.

## ğŸ“Š So sÃ¡nh TÃ­nh nÄƒng

| TÃ­nh nÄƒng | bflow_ai (v1) | bflow_ai_v2 (v2) |
|-----------|---------------|-------------------|
| **RAG Method** | Vector Search (embeddings) | GraphRAG (Knowledge Graph) |
| **Global Questions** | âŒ Yáº¿u | âœ… Máº¡nh |
| **Local Questions** | âœ… Tá»‘t | âœ… Tá»‘t |
| **Query Type** | Similarity search | Entity-based + Community-based |
| **Framework** | Custom pipeline | LangChain |
| **Caching** | Redis + Streaming cache | Redis + Vector cache |
| **Data Format** | JSON files | Text (tá»« JSON convert) |
| **Indexing** | On-the-fly | Pre-built graph |

## ğŸ¯ Khi nÃ o dÃ¹ng Version nÃ o?

### DÃ¹ng bflow_ai (v1) khi:
- CÃ¢u há»i cá»¥ thá»ƒ: "TK 111 lÃ  gÃ¬?", "Háº¡ch toÃ¡n bÃ¡n hÃ ng tháº¿ nÃ o?"
- Cáº§n response nhanh
- CÃ¢u há»i vá» má»™t account cá»¥ thá»ƒ

### DÃ¹ng bflow_ai_v2 (v2) khi:
- CÃ¢u há»i tá»•ng quan: "So sÃ¡nh TT99 vÃ  TT200?", "Tá»•ng quan cÃ¡c chuáº©n má»±c?"
- Cáº§n context toÃ n diá»‡n
- CÃ¢u há»i liÃªn quan Ä‘áº¿n nhiá»u chá»§ Ä‘á»
- Muá»‘n táº­n dá»¥ng knowledge graph

## ğŸ”Œ API Endpoints (Compatible)

Cáº£ 2 backend Ä‘á»u há»— trá»£ endpoint giá»‘ng nhau Ä‘á»ƒ UI cÃ³ thá»ƒ switch:

```
GET /api/ai-bflow/ask
```

**Query Parameters:**
| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `question` | string | required | CÃ¢u há»i |
| `session_id` | string | null | Session ID |
| `chat_type` | string | "thinking" | "thinking" hoáº·c "free" |
| `item_group` | string | "GOODS" | NhÃ³m sáº£n pháº©m |
| `partner_group` | string | "CUSTOMER" | NhÃ³m Ä‘á»‘i tÃ¡c |

**Response:** `text/plain; charset=utf-8` (streaming)

## ğŸ“ Cáº¥u trÃºc Project

```
bflow-ai/
â”œâ”€â”€ bflow_ai/              # Version 1 (Pipeline-based)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agents/        # Multi-agent system
â”‚   â”‚   â”œâ”€â”€ api/           # Endpoints
â”‚   â”‚   â”œâ”€â”€ core/          # Config, LLM, embeddings
â”‚   â”‚   â”œâ”€â”€ pipeline/      # 8-step pipeline
â”‚   â”‚   â””â”€â”€ services/      # Services (cache, search, etc)
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ bflow_ai_v2/           # Version 2 (GraphRAG + LangChain)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ api/           # Endpoints (compatible + v2)
â”‚   â”‚   â”œâ”€â”€ core/          # Config, LangChain LLM
â”‚   â”‚   â”œâ”€â”€ models/        # Pydantic schemas
â”‚   â”‚   â””â”€â”€ services/      # GraphRAG, Vector store
â”‚   â”œâ”€â”€ scripts/           # Convert data, build graph
â”‚   â”œâ”€â”€ ragtest/           # GraphRAG output
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ README_COMMON.md       # File nÃ y
```

## ğŸš€ Quick Start

### 1. CÃ i Ä‘áº·t Ollama vÃ  Pull Models

```bash
# CÃ i Ä‘áº·t Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull models
ollama pull qwen2.5:7b
ollama pull qwen2.5:0.5b
ollama pull nomic-embed-text  # Cho v2
```

### 2. Cháº¡y bflow_ai (v1)

```bash
cd bflow_ai

# Install dependencies (láº§n Ä‘áº§u)
pip install -r requirements.txt

# Cháº¡y server
uvicorn main:app --port 8010
```

### 3. Cháº¡y bflow_ai_v2 (v2)

```bash
cd bflow_ai_v2

# Install dependencies (láº§n Ä‘áº§u)
pip install -r requirements.txt

# Convert data tá»« bflow_ai
python scripts/convert_data.py

# Build knowledge graph (láº§n Ä‘áº§u)
python scripts/build_graph.py

# Cháº¡y server
uvicorn app.main:app --port 8010
```

## ğŸ§ª Testing

### Test báº±ng curl

```bash
# Test endpoint chÃ­nh
curl "http://localhost:8010/api/ai-bflow/ask?question=TK+111+lÃ +gÃ¬?"

# Test vá»›i cÃ¢u há»i global (chá»‰ v2 tráº£ lá»i tá»‘t)
curl "http://localhost:8010/api/ai-bflow/ask?question=So+sÃ¡nh+TT99+vÃ +TT200?"
```

### Test cÃ¢u há»i vÃ­ dá»¥

| Loáº¡i cÃ¢u há»i | VÃ­ dá»¥ | Version khuyáº¿n nghá»‹ |
|-------------|-------|-------------------|
| **Specific** | "TK 111 lÃ  gÃ¬?" | v1 hoáº·c v2 |
| **Specific** | "Háº¡ch toÃ¡n bÃ¡n hÃ ng?" | v1 hoáº·c v2 |
| **Global** | "So sÃ¡nh TT99 vÃ  TT200?" | v2 |
| **Global** | "Tá»•ng quan chuáº©n má»±c káº¿ toÃ¡n?" | v2 |

## ğŸ“Š Monitoring

### bflow_ai (v1)

```bash
# Logs hiá»ƒn thá»‹ pipeline steps:
# [Pipeline] STEP 1: Session Management
# [Pipeline] STEP 2: Building Context
# [Pipeline] STEP 3: Routing to Agent
# ...
```

### bflow_ai_v2 (v2)

```bash
# Health check
curl http://localhost:8010/api/health

# Graph status
curl http://localhost:8010/api/graph/status

# Swagger UI
# Má»Ÿ trÃ¬nh duyá»‡t: http://localhost:8010/docs
```

## âš™ï¸ Configuration

### bflow_ai (v1) - `app/core/config.py`

```python
# Ollama
OLLAMA_HOST = "http://localhost:11434"
LLM_MODEL = "qwen2.5:7b"

# Cache
ENABLE_LLM_CACHE = True
CACHE_TTL = 3600
```

### bflow_ai_v2 (v2) - `app/core/config.py`

```python
# Ollama
OLLAMA_HOST = "http://localhost:11434"
LLM_MODEL = "qwen2.5:7b"
GRAPH_RAG_EMBEDDING_MODEL = "nomic-embed-text"

# GraphRAG
GRAPH_RAG_ENABLED = True
GRAPH_RAG_ROOT = "./ragtest"
```

## ğŸ”§ Troubleshooting

### Ollama khÃ´ng cháº¡y

```bash
# Kiá»ƒm tra Ollama
ollama list

# Start Ollama
ollama serve
```

### Port 8010 Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng

```bash
# TÃ¬m process Ä‘ang dÃ¹ng port
lsof -i :8010

# Kill process
kill -9 <PID>
```

### v2: Graph chÆ°a Ä‘Æ°á»£c build

```bash
cd bflow_ai_v2
python scripts/build_graph.py
```

### v2: KhÃ´ng cÃ³ input files

```bash
cd bflow_ai_v2
python scripts/convert_data.py
```

## ğŸ“ Notes

1. **UI khÃ´ng cáº§n thay Ä‘á»•i** - Cáº£ 2 backend Ä‘á»u dÃ¹ng endpoint `/api/ai-bflow/ask`
2. **Session ID** - ÄÆ°á»£c giá»¯ nguyÃªn format `__SESSION_ID__:{id}\n`
3. **Streaming** - Cáº£ 2 Ä‘á»u tráº£ vá» streaming `text/plain`
4. **CORS** - Cáº£ 2 Ä‘á»u cho phÃ©p táº¥t cáº£ origins

## ğŸš§ Development Roadmap

### bflow_ai (v1)
- âœ… Multi-agent system
- âœ… Hybrid semantic search
- âœ… Streaming cache
- âœ… Pipeline architecture

### bflow_ai_v2 (v2)
- âœ… GraphRAG integration
- âœ… LangChain orchestration
- âœ… Local & Global search
- â³ Agent integration with LangChain Tools
- â³ Advanced graph visualization

## ğŸ“§ Support

For issues or questions:
1. Check logs in terminal
2. Check `/api/health` endpoint
3. Review troubleshooting section above
