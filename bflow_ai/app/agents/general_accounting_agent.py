"""
General Accounting Agent - Chuy√™n gia v·ªÅ k·∫ø to√°n t·ªïng qu√°t

Chuy√™n gia v·ªÅ:
- Nguy√™n t·∫Øc k·∫ø to√°n
- B√°o c√°o t√†i ch√≠nh
- Chu·∫©n m·ª±c k·∫ø to√°n
- C√¢u h·ªèi k·∫ø to√°n chung kh√¥ng c·∫ßn tra c·ª©u t√†i kho·∫£n c·ª• th·ªÉ
"""

from .base import BaseAgent, AgentRole, AgentResult, AgentContext
from ..core.config import settings
from ..core.ollama_client import get_ollama_client
from ..services.stream_utils import stream_by_char


class GeneralAccountingAgent(BaseAgent):
    """
    General Accounting Agent - Chuy√™n gia v·ªÅ k·∫ø to√°n t·ªïng qu√°t

    X·ª≠ l√Ω:
    - C√¢u h·ªèi v·ªÅ nguy√™n t·∫Øc, kh√°i ni·ªám k·∫ø to√°n
    - B√°o c√°o t√†i ch√≠nh
    - Chu·∫©n m·ª±c k·∫ø to√°n
    - C√°c c√¢u h·ªèi k·∫ø to√°n kh√¥ng c·∫ßn tra c·ª©u c·ª• th·ªÉ
    """

    def __init__(self):
        super().__init__()
        self._init_tools()

    @property
    def name(self) -> str:
        return "GENERAL_ACCOUNTING"

    @property
    def role(self) -> AgentRole:
        return AgentRole.GENERALIST

    @property
    def description(self) -> str:
        return "Chuy√™n gia v·ªÅ k·∫ø to√°n t·ªïng qu√°t. Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ nguy√™n t·∫Øc, b√°o c√°o t√†i ch√≠nh, chu·∫©n m·ª±c k·∫ø to√°n."

    def _init_tools(self):
        """ƒêƒÉng k√Ω tools - agent n√†y kh√¥ng c√≥ RAG tools"""
        # Agent n√†y d√πng SLM knowledge n√™n kh√¥ng c√≥ tools ph·ª©c t·∫°p
        pass

    def can_handle(self, context: AgentContext) -> tuple[bool, float]:
        """
        Ki·ªÉm tra agent c√≥ th·ªÉ x·ª≠ l√Ω query kh√¥ng.

        ƒê√¢y l√† fallback agent - lu√¥n return True v·ªõi confidence th·∫•p.
        C√°c chuy√™n gia kh√°c s·∫Ω c√≥ confidence cao h∆°n n·∫øu match domain.
        """
        return True, 0.3

    def execute(self, context: AgentContext) -> AgentResult:
        """Th·ª±c thi query"""
        system_prompt = """B·∫°n l√† chuy√™n gia k·∫ø to√°n Vi·ªát Nam. LU√îN tr·∫£ l·ªùi b·∫±ng TI·∫æNG VI·ªÜT.
Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ k·∫ø to√°n d·ª±a tr√™n ki·∫øn th·ª©c chuy√™n m√¥n c·ªßa b·∫°n.
Tr√¨nh b√†y r√µ r√†ng, c√≥ c·∫•u tr√∫c, d·ªÖ hi·ªÉu."""

        try:
            client = get_ollama_client()

            # Build messages with history
            messages = [{"role": "system", "content": system_prompt}]
            if context.history:
                messages.extend(context.history)
            messages.append({"role": "user", "content": context.question})

            response = client.chat(
                model=settings.GENERATION_MODEL,
                messages=messages,
                options=settings.OLLAMA_OPTIONS,
                stream=False
            )
            content = response.get("message", {}).get("content", "")

            return AgentResult(
                agent_name=self.name,
                content=content,
                confidence=0.6,
                sources=["SLM Knowledge"]
            )
        except Exception as e:
            print(f"[GeneralAccountingAgent Error] {e}")
            return AgentResult(
                agent_name=self.name,
                content="Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë. Vui l√≤ng th·ª≠ l·∫°i sau.",
                confidence=0.3
            )

    def stream_execute(self, context: AgentContext):
        """Execute v·ªõi streaming response"""
        system_prompt = """B·∫°n l√† chuy√™n gia k·∫ø to√°n Vi·ªát Nam. LU√îN tr·∫£ l·ªùi b·∫±ng TI·∫æNG VI·ªÜT.
Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ k·∫ø to√°n d·ª±a tr√™n ki·∫øn th·ª©c chuy√™n m√¥n c·ªßa b·∫°n.
Tr√¨nh b√†y r√µ r√†ng, c√≥ c·∫•u tr√∫c, d·ªÖ hi·ªÉu."""

        try:
            client = get_ollama_client()

            # Build messages with history
            messages = [{"role": "system", "content": system_prompt}]
            if context.history:
                messages.extend(context.history)
            messages.append({"role": "user", "content": context.question})

            stream = client.chat(
                model=settings.GENERATION_MODEL,
                messages=messages,
                options=settings.OLLAMA_OPTIONS,
                stream=True
            )

            for char in stream_by_char(stream):
                yield char

        except Exception as e:
            print(f"[GeneralAccountingAgent Stream Error] {e}")
            yield "Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë. Vui l√≤ng th·ª≠ l·∫°i sau."


class GeneralFreeAgent(BaseAgent):
    """
    General Free Agent - Tr·ª£ l√Ω AI t·ªïng qu√°t

    X·ª≠ l√Ω:
    - C√¢u h·ªèi kh√¥ng li√™n quan k·∫ø to√°n
    - Chat t·ª± do
    """

    def __init__(self):
        super().__init__()

    @property
    def name(self) -> str:
        return "GENERAL_FREE"

    @property
    def role(self) -> AgentRole:
        return AgentRole.GENERALIST

    @property
    def description(self) -> str:
        return "Tr·ª£ l√Ω AI th√¥ng minh. Tr·∫£ l·ªùi c√°c c√¢u h·ªèi t·ª± do, kh√¥ng li√™n quan k·∫ø to√°n."

    def can_handle(self, context: AgentContext) -> tuple[bool, float]:
        """
        Ki·ªÉm tra agent c√≥ th·ªÉ x·ª≠ l√Ω query kh√¥ng.

        D√πng LLM ƒë·ªÉ ph√¢n lo·∫°i: Accounting vs General chat.
        """
        # G·ªçi LLM ph√¢n lo·∫°i (lightweight, nhanh)
        is_general = self._is_general_chat(context.question)

        if is_general:
            # General chat - confidence cao
            return True, 0.95

        # Fallback - confidence th·∫•p nh·∫•t
        return True, 0.05

    def _is_general_chat(self, question: str) -> bool:
        """
        D√πng LLM nh·ªè ƒë·ªÉ ph√¢n lo·∫°i c√¢u h·ªèi c√≥ ph·∫£i general chat kh√¥ng.

        Tr·∫£ v·ªÅ True n·∫øu l√† general chat, False n·∫øu li√™n quan k·∫ø to√°n.
        """
        from ..core.ollama_client import get_ollama_client
        from ..core.config import settings

        client = get_ollama_client()

        prompt = f"""Ph√¢n lo·∫°i c√¢u h·ªèi sau. CH·ªà TR·∫¢ V·ªÄ "YES" ho·∫∑c "NO".

C√¢u h·ªèi: {question}

YES n·∫øu:
- Chat x√£ giao, ch√†o h·ªèi, c·∫£m ∆°n, h·ªèi thƒÉm s·ª©c kh·ªèe
- Kh√¥ng li√™n quan ƒë·∫øn k·∫ø to√°n, t√†i kho·∫£n, h·∫°ch to√°n
- C√¢u h·ªèi ƒë·ªùi s·ªëng, t√¨nh c·∫£m, s·ªü th√≠ch

NO n·∫øu:
- H·ªèi v·ªÅ k·∫ø to√°n, t√†i kho·∫£n, h·∫°ch to√°n, b√°o c√°o t√†i ch√≠nh
- H·ªèi v·ªÅ nghi·ªáp v·ª• k·∫ø to√°n
- Ch·ª©a t·ª´ kh√≥a chuy√™n ng√†nh k·∫ø to√°n

Ch·ªâ tr·∫£: YES ho·∫∑c NO"""

        try:
            response = client.chat(
                model=settings.CLASSIFIER_MODEL,  # qwen2.5:0.5b - nhanh
                messages=[{"role": "user", "content": prompt}],
                options={"num_predict": 3, "temperature": 0},  # Ch·ªâ c·∫ßn YES/NO
                stream=False
            )

            result = response.get("message", {}).get("content", "").strip().upper()

            print(f"[GeneralFreeAgent] LLM classification: {result} for question: {question[:50]}...")

            return "YES" in result

        except Exception as e:
            print(f"[GeneralFreeAgent] Classification error: {e}")
            # Fallback: n·∫øu LLM fail, assume general (an to√†n h∆°n)
            return True

    def execute(self, context: AgentContext) -> AgentResult:
        """Th·ª±c thi query"""
        system_prompt = """B·∫°n l√† ng∆∞·ªùi b·∫°n th√¢n thi·ªán, hay n√≥i chuy·ªán.

QUY T·∫ÆC:
1. PH·∫¢I ƒë·ªçc to√†n b·ªô l·ªãch s·ª≠ tr√≤ chuy·ªán tr∆∞·ªõc khi tr·∫£ l·ªùi
2. Hi·ªÉu ng·ªØ c·∫£nh r·ªìi m·ªõi ph·∫£n h·ªìi cho ƒë√∫ng
3. Th∆∞·ªùng xuy√™n h·ªèi l·∫°i ng∆∞·ªùi d√πng ƒë·ªÉ duy tr√¨ h·ªôi tho·∫°i
4. Cho ph√©p d√πng emoji üòä
5. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·ª± nhi√™n nh∆∞ chat v·ªõi b·∫°n b√®
6. Lu√¥n d√πng Ti·∫øng Vi·ªát"""

        try:
            client = get_ollama_client()

            messages = [{"role": "system", "content": system_prompt}]
            if context.history:
                messages.extend(context.history)
            messages.append({"role": "user", "content": context.question})

            response = client.chat(
                model=settings.GENERATION_MODEL,
                messages=messages,
                options=settings.GENERAL_FREE_OPTIONS,  # D√πng options ri√™ng cho free chat
                stream=False
            )
            content = response.get("message", {}).get("content", "")

            return AgentResult(
                agent_name=self.name,
                content=content,
                confidence=0.5,
                sources=["SLM General Knowledge"]
            )
        except Exception as e:
            print(f"[GeneralFreeAgent Error] {e}")
            return AgentResult(
                agent_name=self.name,
                content="Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë. Vui l√≤ng th·ª≠ l·∫°i sau.",
                confidence=0.2
            )

    def stream_execute(self, context: AgentContext):
        """Execute v·ªõi streaming response"""
        system_prompt = """B·∫°n l√† ng∆∞·ªùi b·∫°n th√¢n thi·ªán, hay n√≥i chuy·ªán.

QUY T·∫ÆC:
1. PH·∫¢I ƒë·ªçc to√†n b·ªô l·ªãch s·ª≠ tr√≤ chuy·ªán tr∆∞·ªõc khi tr·∫£ l·ªùi
2. Hi·ªÉu ng·ªØ c·∫£nh r·ªìi m·ªõi ph·∫£n h·ªìi cho ƒë√∫ng
3. Th∆∞·ªùng xuy√™n h·ªèi l·∫°i ng∆∞·ªùi d√πng ƒë·ªÉ duy tr√¨ h·ªôi tho·∫°i
4. Cho ph√©p d√πng emoji üòä
5. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·ª± nhi√™n nh∆∞ chat v·ªõi b·∫°n b√®
6. Lu√¥n d√πng Ti·∫øng Vi·ªát"""

        try:
            client = get_ollama_client()

            messages = [{"role": "system", "content": system_prompt}]
            if context.history:
                messages.extend(context.history)
            messages.append({"role": "user", "content": context.question})

            stream = client.chat(
                model=settings.GENERATION_MODEL,
                messages=messages,
                options=settings.GENERAL_FREE_OPTIONS,  # D√πng options ri√™ng cho free chat
                stream=True
            )

            for char in stream_by_char(stream):
                yield char

        except Exception as e:
            print(f"[GeneralFreeAgent Stream Error] {e}")
            yield "Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë. Vui l√≤ng th·ª≠ l·∫°i sau."
