import json
import os
import faiss
import ollama
import numpy as np
import re
from sentence_transformers import SentenceTransformer

# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---
# L∆∞u √Ω: C·∫ßn ƒë·∫£m b·∫£o file JSON n·∫±m ƒë√∫ng v·ªã tr√≠ n√†y so v·ªõi file ch·∫°y
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_FILE_NAME = os.path.join(BASE_DIR, 'services', 'account_deter_json', '80785ce8-f138-48b8-b7fa-5fb1971fe204.json')


class HandleJsonFile:
    @staticmethod
    def read(file_path):
        if not os.path.exists(file_path):
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu t·∫°i: {file_path}")
            return []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc file JSON: {e}")
            return []


# --- KH·ªûI T·∫†O MODEL & VECTOR DB ---
# (Ph·∫ßn n√†y s·∫Ω ch·∫°y 1 l·∫ßn khi server start)
print("‚è≥ ƒêang load model SentenceTransformer...")
try:
    _model = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')
    print("‚úÖ Model loaded th√†nh c√¥ng!")

    # Load d·ªØ li·ªáu
    # ƒêi·ªÅu ch·ªânh ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi t√πy theo c·∫•u tr√∫c th∆∞ m·ª•c th·ª±c t·∫ø c·ªßa b·∫°n
    # ·ªû ƒë√¢y gi·∫£ ƒë·ªãnh file json n·∫±m ·ªü ../data/...
    accounting_data = HandleJsonFile.read(DATA_FILE_NAME)

    accounting_texts = []
    if accounting_data:
        for item in accounting_data:
            nv = item.get('nghiep_vu', '')
            mt = item.get('mo_ta_chi_tiet', '')
            bct_raw = item.get('bo_chung_tu', [])
            bct_str = ", ".join(bct_raw) if isinstance(bct_raw, list) else str(bct_raw)
            text_embed = f"Nghi·ªáp v·ª•: {nv}. M√¥ t·∫£: {mt}. Ch·ª©ng t·ª´ bao g·ªìm: {bct_str}"
            accounting_texts.append(text_embed)

    if accounting_texts:
        print(f"‚è≥ ƒêang t·∫°o vector database cho {len(accounting_texts)} nghi·ªáp v·ª•...")
        account_embedding = _model.encode(accounting_texts, convert_to_numpy=True, show_progress_bar=False)
        account_dimension = account_embedding.shape[1]
        account_index = faiss.IndexFlatL2(account_dimension)
        account_index.add(account_embedding)
        print("‚úÖ FAISS index ƒë√£ s·∫µn s√†ng!")
    else:
        account_index = None
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ t·∫°o index.")

except Exception as e:
    print(f"‚ùå L·ªói kh·ªüi t·∫°o Model/Index: {e}")
    _model = None
    account_index = None
    accounting_data = []


class RagAccounting:
    @staticmethod
    def rag_accounting(user_input: str, top_k: int = 1):
        """
        H√†m Generator tr·∫£ v·ªÅ t·ª´ng token vƒÉn b·∫£n.
        """
        if not account_index or not _model:
            yield "H·ªá th·ªëng ƒëang kh·ªüi ƒë·ªông ho·∫∑c ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng th·ª≠ l·∫°i sau."
            return

        try:
            # 1. T√¨m ki·∫øm context
            print('ƒêang t√¨m context')
            user_embedding = _model.encode([user_input], convert_to_numpy=True, show_progress_bar=False)
            D, I = account_index.search(np.array(user_embedding).astype('float32'), k=top_k)

            results = []
            for idx, dist in zip(I[0], D[0]):
                if idx < 0: continue
                # Ki·ªÉm tra bi√™n an to√†n
                if idx < len(accounting_data):
                    item = accounting_data[idx]
                    results.append(item)

            if not results:
                yield "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y nghi·ªáp v·ª• k·∫ø to√°n ph√π h·ª£p trong c∆° s·ªü d·ªØ li·ªáu."
                return

            # 2. G·ªçi LLM Streaming
            yield from RagAccounting.synthesize_answer(user_input, results)

        except Exception as e:
            print(f"‚ùå L·ªói RAG: {e}")
            yield f"C√≥ l·ªói x·∫£y ra: {str(e)}"

    @staticmethod
    def synthesize_answer(user_query, retrieved_data):
        context_str = json.dumps(retrieved_data, indent=2, ensure_ascii=False)

        # Prompt ƒë·ªãnh d·∫°ng Markdown/Text (Kh√¥ng d√πng HTML)
        prompt = f"""
            B·∫°n l√† K·∫ø to√°n tr∆∞·ªüng chuy√™n nghi·ªáp. D·ª±a v√†o d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y ƒë·ªÉ h∆∞·ªõng d·∫´n h·∫°ch to√°n.
            
            [D·ªÆ LI·ªÜU T√åM ƒê∆Ø·ª¢C]:
            {context_str}
            
            [C√ÇU H·ªéI]: "{user_query}"
            
            [Y√äU C·∫¶U TR·∫¢ L·ªúI]:
            1. KH√îNG d√πng th·∫ª HTML.
            2. Tr√¨nh b√†y b·∫±ng vƒÉn b·∫£n (Markdown) r√µ r√†ng, chuy√™n nghi·ªáp.
            3. S·ª≠ d·ª•ng c√°c k√Ω t·ª± nh∆∞ (-, +, *, >) ho·∫∑c Emoji ƒë·ªÉ ph√¢n t√°ch √Ω.
            4. C·∫•u tr√∫c c√¢u tr·∫£ l·ªùi b·∫Øt bu·ªôc:
            
               üéØ NGHI·ªÜP V·ª§: [T√™n nghi·ªáp v·ª•]
            
               üìÑ M√î T·∫¢: [M√¥ t·∫£ chi ti·∫øt]
            
               üìÇ B·ªò CH·ª®NG T·ª™ B·∫ÆT BU·ªòC:
                 - [Li·ªát k√™ c√°c ch·ª©ng t·ª´...]
            
               üí∞ ƒê·ªäNH KHO·∫¢N:
                 * N·ª£ TK [S·ªë TK] - [T√™n TK]
                 * C√≥ TK [S·ªë TK] - [T√™n TK]
            
               üí° L∆ØU √ù & GI·∫¢I TH√çCH:
                 > [N·ªôi dung ghi ch√∫/tham chi·∫øu]
            
            B·∫Øt ƒë·∫ßu tr·∫£ l·ªùi ngay:
        """

        model = "qwen2.5:1.5b"  # Ho·∫∑c model b·∫°n ƒëang d√πng

        # stream=True ƒë·ªÉ nh·∫≠n t·ª´ng token
        client = ollama.Client(host='http://mis_ollama:11434')
        stream = client.generate(
            model=model,
            prompt=prompt,
            options={'temperature': 0.2},
            stream=True
        )

        for chunk in stream:
            content = chunk.get('response', '')
            if content:
                yield content

        print('Done')
        